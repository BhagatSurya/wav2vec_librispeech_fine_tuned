{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSxBVaTz8Vptv3hFFf6AnY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhagatSurya/wav2vec_librispeech_fine_tuned/blob/main/wav2vec_librispeech_asr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**@Bhagat Surya**"
      ],
      "metadata": {
        "id": "5mQMmz0zHkJe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAQn-6ymDxfw"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==1.18.3\n",
        "!pip install transformers==4.17.0\n",
        "!pip install jiwer\n",
        "\n",
        "from datasets import ClassLabel\n",
        "import random \n",
        "import pandas as pd \n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "train =load_dataset(\"librispeech_asr\",\"clean\",split=\"train.100\")\n",
        "test = load_dataset(\"librispeech_asr\",\"clean\",split=\"test\")\n",
        "\n",
        "train= train.remove_columns([\"speaker_id\",\"chapter_id\",\"id\"])\n",
        "test = test.remove_columns([\"speaker_id\",\"chapter_id\",\"id\"])\n",
        "\n",
        "def extract_all_character(batch):\n",
        "  all_text_in_file = \" \".join(batch[\"text\"])\n",
        "  vocabulary = list(set(all_text_in_file))\n",
        "  return {\"vocabulary\":[vocabulary],\"text\":[all_text_in_file]}\n",
        "\n",
        "\n",
        "voab_list = list(set(train_voab[\"vocabulary\"][0]) | set(test_voab[\"vocabulary\"][0]))\n",
        "\n",
        "vocabulary_dict = {i:k for k,i in enumerate(voab_list)}\n",
        "\n",
        "vocabulary_dict[\"|\"] = vocabulary_dict[\" \"]\n",
        "del vocabulary_dict[\" \"]\n",
        "\n",
        "vocabulary_dict[\"[UNK]\"] = len(vocabulary_dict)\n",
        "vocabulary_dict[\"[PAD\"] =len(vocabulary_dict)\n",
        "\n",
        "import json\n",
        "with open(\"vocabulary.json\",\"w\") as vocabulary_file:\n",
        "  json.dump(vocabulary_dict,vocabulary_file)\n",
        "\n",
        "#CTC Token\n",
        "from transformers import  Wav2Vec2CTCTokenizer\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"./vocabulary.json\",unk_token=\"[UNK]\",pad_token=\"[PAD]\",word_delimiter_token=\"|\")\n",
        "\n",
        "#Feature extraction\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "feature_extraction = Wav2Vec2FeatureExtractor(feature_size=1,sampling_rate=16000,padding_value=0.0,do_normalize=True,return_attention_mask=False)\n",
        "\n",
        "from transformers import Wav2Vec2Processor\n",
        "processor = Wav2Vec2Processor(tokenizer=tokenizer,feature_extractor=feature_extraction)\n",
        "\n",
        "dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-English-librispeech_asr\"\n",
        "\n",
        "processor.save_pretrained(dir)\n",
        "\n",
        "\n",
        "rand_int = random.randint(0, len(train))\n",
        "print(\"Target text:\", train[rand_int][\"text\"])\n",
        "print(\"Input array shape:\", np.asarray(train[rand_int][\"audio\"]).shape)\n",
        "print(\"Sampling rate:\", train[rand_int][\"audio\"][\"sampling_rate\"])\n",
        "\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "train = train.map(prepare_dataset, remove_columns=train.column_names, num_proc=4)\n",
        "test = test.map(prepare_dataset, remove_columns=test.column_names,num_proc=4)\n",
        "\n",
        "\n",
        "import torch \n",
        "from dataclasses import dataclass,field\n",
        "from typing import  Any, Dict, List, Optional, Union\n",
        "@dataclass\n",
        "class DatacollatorCTCwithPadding():\n",
        "  processor: Wav2Vec2Processor\n",
        "  padding: Union[bool,str] = True\n",
        "  max_length: Optional[int] =  None\n",
        "  max_length_labels: Optional[int] = None\n",
        "  pad_to_multiple_of: Optional[int] =  None\n",
        "  pad_to_multiple_of_lables: Optional[int] = None\n",
        "\n",
        "  def __call__(self,features:List[Dict[str,Union[List[int],torch.Tensor]]]) -> Dict[str,torch.Tensor]:\n",
        "    input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "    label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "    batch =  self.processor.pad(\n",
        "        input_features,\n",
        "        padding= self.padding,\n",
        "        max_length = self.max_length,\n",
        "        pad_to_multiple_of =  self.pad_to_multiple_of,\n",
        "        return_tensors = \"pt\"\n",
        "\n",
        "    )\n",
        "\n",
        "    with self.processor.as_target_processor():\n",
        "      labels_batch = self.processor.pad(\n",
        "          label_features,\n",
        "          padding = self.padding,\n",
        "          max_length = self.max_length_labels,\n",
        "          pad_to_multiple_of = self.pad_to_multiple_of_lables,\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "\n",
        "      lables = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1),-100)\n",
        "\n",
        "      batch[\"labels\"] = lables\n",
        "      \n",
        "      return batch\n",
        "\n",
        "\n",
        "data_collator = DatacollatorCTCwithPadding(processor=processor, padding=True)\n",
        "\n",
        "\n",
        "from datasets.load import load_metric\n",
        "wer_metric = load_metric(\"wer\")\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "\n",
        "#model \n",
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", \n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=dir,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=32,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=30,\n",
        "  fp16=True,\n",
        "  gradient_checkpointing=True, \n",
        "  save_steps=500,\n",
        "  eval_steps=500,\n",
        "  logging_steps=500,\n",
        "  learning_rate=1e-4,\n",
        "  weight_decay=0.005,\n",
        "  warmup_steps=1000,\n",
        "  save_total_limit=2,\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-English-librispeech_asr\").to(\"cuda\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-English-librispeech_asr\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import number\n",
        "def show_random_elemnts(dataset,elemnts_number=10):\n",
        "  assert elemnts_number <= len(dataset),\"please enter valid\"\n",
        "  sample =[]\n",
        "  for _ in range(elemnts_number):\n",
        "    pick = random.randint(0,len(dataset)-1)\n",
        "    while pick in sample:\n",
        "      pick = random.randint(0,len(dataset)-1)\n",
        "    sample.append(pick)\n",
        "  \n",
        "  df = pd.DataFrame(dataset[sample])\n",
        "  display(HTML(df.to_html()))\n",
        "\n",
        "show_random_elemnts(train.remove_columns([\"file\",\"audio\"]))\n",
        "\n",
        "def map_to_result(batch):\n",
        "  with torch.no_grad():\n",
        "    input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
        "    logits = model(input_values).logits\n",
        "\n",
        "  pred_ids = torch.argmax(logits, dim=-1)\n",
        "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
        "  batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
        "  \n",
        "  return batch\n",
        "\n",
        "results = test.map(map_to_result, remove_columns=test.column_names)\n",
        "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))\n",
        "\n",
        "\n",
        "show_random_elemnts(results.remove_columns([\"speech\", \"sampling_rate\"]))"
      ],
      "metadata": {
        "id": "b8-xi4ADFHqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DByH5uKkmuGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}